import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import os
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from openai import OpenAI

# Font verification function
def setup_korean_font():
    # Windows (Local)
    if os.name == 'nt':
        plt.rc('font', family='Malgun Gothic')
        plt.rc('axes', unicode_minus=False)
    # Linux (Streamlit Cloud)
    else:
        # Check if NanumGothic is installed or download it
        font_path = "NanumGothic.ttf"
        if not os.path.exists(font_path):
            import requests
            url = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Regular.ttf"
            response = requests.get(url)
            with open(font_path, "wb") as f:
                f.write(response.content)
        
        fm.fontManager.addfont(font_path)
        plt.rc('font', family='NanumGothic')
        plt.rc('axes', unicode_minus=False)

setup_korean_font()

# Page Config
st.set_page_config(
    page_title="ì„œìš¸ì‹œ ìƒê¶Œ ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸",
    page_icon="ğŸ™ï¸",
    layout="wide"
)

# Title
st.title("ğŸ™ï¸ ì„œìš¸ì‹œ ìƒê¶Œ ë¶„ì„ ë° ë§¤ì¶œ ì˜ˆì¸¡ AI")
st.markdown("ì„œìš¸ì‹œ ìƒê¶Œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¶œì„ ì˜ˆì¸¡í•˜ê³ , AIì™€ ëŒ€í™”í•˜ë©° ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì–´ë³´ì„¸ìš”.")

# Sidebar
st.sidebar.header("ì„¤ì • (Settings)")
api_key = st.sidebar.text_input("OpenAI API Key", type="password", help="ê²°ê³¼ í•´ì„ì„ ìœ„í•´ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

# Data Loading function
@st.cache_data
def load_data():
    try:
        # Load datasets
        df_pop = pd.read_csv("ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ê¸¸ë‹¨ìœ„ì¸êµ¬-ìƒê¶Œ).csv", encoding='cp949')
        df_change = pd.read_csv("ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ìƒê¶Œë³€í™”ì§€í‘œ-ìƒê¶Œ).csv", encoding='cp949')
        # Handle files in data/ folder or current folder if migrated
        try:
            df_store = pd.read_csv("data/ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì í¬-ìƒê¶Œ)_2024ë…„.csv", encoding='cp949')
        except FileNotFoundError:
             df_store = pd.read_csv("ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì í¬-ìƒê¶Œ)_2024ë…„.csv", encoding='cp949')
        
        try:
            df_sales = pd.read_csv("data/ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì¶”ì •ë§¤ì¶œ-ìƒê¶Œ)_2024ë…„.csv", encoding='cp949')
        except FileNotFoundError:
            df_sales = pd.read_csv("ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì¶”ì •ë§¤ì¶œ-ìƒê¶Œ)_2024ë…„.csv", encoding='cp949')

        # Merge Data
        # Strategy: Merge basic info first.
        # Use inner join to keep only matching records across all datasets
        
        # 1. Pop + Change
        df_merged = pd.merge(df_pop, df_change, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ', 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…', 'ìƒê¶Œ_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…'], how='inner')
        
        # 2. Add Store info
        # Store data might have multiple rows per district (different service codes). 
        # For simplicity in this regression, let's aggregate store counts per district/quarter
        # Or better, filter for a specific service code if asked, but here we want general 'District' analysis.
        # Aggregating store metrics by district and quarter
        store_agg = df_store.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ']).agg({
            'ì í¬_ìˆ˜': 'sum',
            'í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜': 'sum',
            'ê°œì—…_ì í¬_ìˆ˜': 'sum',
            'íì—…_ì í¬_ìˆ˜': 'sum'
        }).reset_index()
        
        df_merged = pd.merge(df_merged, store_agg, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ'], how='inner')
        
        # 3. Add Sales info (Target)
        # Sales data also split by service code. We should aggregate total sales for the district for a holistic view
        # OR allow user to select service code.
        # Let's aggregate for now to predict "Total District Sales"
        sales_agg = df_sales.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ']).agg({
            'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
            'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜': 'sum',
            'ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
            'ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡': 'sum'
        }).reset_index()
        
        df_merged = pd.merge(df_merged, sales_agg, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ'], how='inner')
        
        return df_merged
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

df = load_data()

if df is not None:
    st.sidebar.success("ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (Row: " + str(len(df)) + ")")
    
    # ----------------------------------------
    # Filters
    # ----------------------------------------
    st.sidebar.subheader("ë°ì´í„° í•„í„°")
    quarters = sorted(df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].unique())
    selected_quarter = st.sidebar.selectbox("ë¶„ê¸° ì„ íƒ", quarters, index=len(quarters)-1)
    
    # Filter Data for Display/Analysis context (optional, maybe we want to train on ALL and predict/analyze specific)
    # Let's keep all data for training to get better model, but highlight selected data.
    
    # Selection of Features for Regression
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    # Remove ID cols and Target cols from features
    exclude_cols = ['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ', 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 'ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡']
    feature_candidates = [c for c in numeric_cols if c not in exclude_cols]
    
    # Default features
    default_features = ['ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì í¬_ìˆ˜', 'í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ']
    default_features = [f for f in default_features if f in feature_candidates]
    
    selected_features = st.multiselect("í•™ìŠµ í•  Feature ì„ íƒ", feature_candidates, default=default_features)
    
    target_col = 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'
    
    if st.button("ë¶„ì„ ë° ì˜ˆì¸¡ ì‹¤í–‰ (Run Analysis)"):
        if not selected_features:
            st.warning("Featureë¥¼ ìµœì†Œ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
            st.stop()
            
        # ----------------------------------------
        # Regression Analysis
        # ----------------------------------------
        X = df[selected_features]
        y = df[target_col]
        
        # Simple fillna just in case
        X = X.fillna(0)
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        
        # Metrics
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        
        st.divider()
        st.header("ğŸ“Š íšŒê·€ ë¶„ì„ ê²°ê³¼")
        
        col1, col2 = st.columns(2)
        col1.metric("R-Squared (ê²°ì •ê³„ìˆ˜)", f"{r2:.4f}")
        col2.metric("MSE (í‰ê· ì œê³±ì˜¤ì°¨)", f"{mse:,.0f}")
        
        # ----------------------------------------
        # Visualization
        # ----------------------------------------
        st.subheader("1. Feature Importance (íšŒê·€ ê³„ìˆ˜)")
        coef_df = pd.DataFrame({'Feature': selected_features, 'Coefficient': model.coef_})
        coef_df = coef_df.sort_values(by='Coefficient', ascending=False)
        
        fig_coef, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(data=coef_df, x='Coefficient', y='Feature', ax=ax, palette='viridis')
        ax.set_title("ê° ë³€ìˆ˜ê°€ ë§¤ì¶œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„")
        st.pyplot(fig_coef)
        
        st.subheader("2. Actual vs Predicted Sales")
        fig_scatter, ax = plt.subplots(figsize=(10, 6))
        sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
        # Ideal line
        min_val = min(y_test.min(), y_pred.min())
        max_val = max(y_test.max(), y_pred.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--')
        ax.set_xlabel("ì‹¤ì œ ë§¤ì¶œ")
        ax.set_ylabel("ì˜ˆì¸¡ ë§¤ì¶œ")
        ax.set_title("ì‹¤ì œ ë§¤ì¶œ vs ì˜ˆì¸¡ ë§¤ì¶œ ì‚°ì ë„")
        st.pyplot(fig_scatter)
        
        # ----------------------------------------
        # Chat Interface Integration
        # ----------------------------------------
        st.divider()
        st.header("ğŸ¤– AI ë¶„ì„ ê²°ê³¼ í•´ì„")
        st.caption("ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ AIì™€ ëŒ€í™”í•´ë³´ì„¸ìš”.")

        # Store analysis context in session state to pass to LLM
        analysis_summary = f"""
        **íšŒê·€ ë¶„ì„ ìš”ì•½**:
        - íƒ€ê²Ÿ ë³€ìˆ˜: ìƒê¶Œ ì›” ë§¤ì¶œì•¡
        - ì‚¬ìš© ë³€ìˆ˜: {', '.join(selected_features)}
        - ëª¨ë¸ ì„±ëŠ¥ (R2): {r2:.4f}
        
        **ì£¼ìš” ë³€ìˆ˜ ì˜í–¥ë„ (ê³„ìˆ˜)**:
        {coef_df.to_string(index=False)}
        """
        
        if "messages" not in st.session_state:
            st.session_state.messages = []
            # Initial system message
            st.session_state.messages.append({
                "role": "system", 
                "content": f"ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì„œìš¸ì‹œ ìƒê¶Œ ë¶„ì„ ë°ì´í„°ì— ëŒ€í•œ íšŒê·€ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n[ë¶„ì„ ê²°ê³¼ ë°ì´í„°]\n{analysis_summary}"
            })
            # Add initial AI greeting
            st.session_state.messages.append({"role": "assistant", "content": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”."})

        # Display chat history
        for msg in st.session_state.messages:
            if msg["role"] != "system":
                with st.chat_message(msg["role"]):
                    st.write(msg["content"])

        # Chat Input
        if prompt := st.chat_input("ì´ ê²°ê³¼ê°€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€ìš”?"):
            if not api_key:
                st.error("OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # Add user message
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.write(prompt)
                
                # Stream response
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    full_response = ""
                    try:
                        client = OpenAI(api_key=api_key)
                        
                        # Call API
                        # Filter system message + last N messages to fit context if needed, but usually fine for simple chats
                        stream = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[
                                {"role": m["role"], "content": m["content"]}
                                for m in st.session_state.messages
                            ],
                            stream=True,
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content is not None:
                                full_response += chunk.choices[0].delta.content
                                message_placeholder.write(full_response + "â–Œ")
                        
                        message_placeholder.write(full_response)
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                        
                    except Exception as e:
                        st.error(f"Error: {e}")
                        
else:
    st.info("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
