import streamlit as st
import sys
import io

# Streamlit Cloud ë°°í¬ ì‹œ í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ í•´ê²°
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from openai import OpenAI

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë³´ìŠ¤í„´ ì§‘ê°’ ë¶„ì„ & ì±—ë´‡", layout="wide")

# ì œëª©
st.title("ğŸ¡ ë³´ìŠ¤í„´ ì§‘ê°’ ë°ì´í„° íšŒê·€ ë¶„ì„ ë° AI ì±—ë´‡")

# ì‚¬ì´ë“œë°”: OpenAI API í‚¤ ì…ë ¥
st.sidebar.header("ì„¤ì •")
api_key = st.sidebar.text_input("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ìºì‹± ì ìš©)
@st.cache_data
def load_data():
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]
    feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
    df = pd.DataFrame(data, columns=feature_names)
    df['PRICE'] = target
    return df

# ë©”ì¸ ë¶„ì„ ë¡œì§
try:
    df = load_data()

    # ë°ì´í„° ë¶„í• 
    X = df.drop('PRICE', axis=1)
    y = df['PRICE']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ëª¨ë¸ í•™ìŠµ
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # ì„±ëŠ¥ í‰ê°€
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # ë¶„ì„ ê²°ê³¼ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„± (ì±—ë´‡ ì»¨í…ìŠ¤íŠ¸ìš©)
    coefficients = pd.Series(model.coef_, index=X.columns).sort_values(ascending=False)
    analysis_summary = f"""
    [ë¶„ì„ ìš”ì•½]
    - ëª¨ë¸: ì„ í˜• íšŒê·€ (Linear Regression)
    - MSE (í‰ê·  ì œê³± ì˜¤ì°¨): {mse:.2f}
    - R2 Score (ê²°ì • ê³„ìˆ˜): {r2:.2f}
    
    [ì£¼ìš” ë³€ìˆ˜ ì˜í–¥ë„ (ê³„ìˆ˜)]
    ìƒìœ„ 3ê°œ ì–‘ì˜ ìƒê´€ê´€ê³„:
    {coefficients.head(3).to_string()}
    
    ìƒìœ„ 3ê°œ ìŒì˜ ìƒê´€ê´€ê³„:
    {coefficients.tail(3).to_string()}
    """

    # ë ˆì´ì•„ì›ƒ: 2ë‹¨ ì»¬ëŸ¼
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ğŸ“Š ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”")
        
        # 1. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        st.markdown("### ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
        fig_corr, ax_corr = plt.subplots(figsize=(10, 8))
        sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm', ax=ax_corr)
        st.pyplot(fig_corr)

        # 2. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’
        st.markdown("### ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ (Test Set)")
        fig_scat, ax_scat = plt.subplots(figsize=(8, 6))
        ax_scat.scatter(y_test, y_pred, alpha=0.7)
        ax_scat.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
        ax_scat.set_xlabel("Actual Price")
        ax_scat.set_ylabel("Predicted Price")
        ax_scat.set_title(f"R2 Score: {r2:.2f}")
        st.pyplot(fig_scat)

        st.info(analysis_summary)

    with col2:
        st.subheader("ğŸ’¬ AI ë°ì´í„° ë¶„ì„ê°€")
        
        if not api_key:
            st.warning("ì‚¬ì´ë“œë°”ì— OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            client = OpenAI(api_key=api_key)

            # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
            if "messages" not in st.session_state:
                st.session_state.messages = [
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ë³´ìŠ¤í„´ ì§‘ê°’ ë°ì´í„°ì˜ íšŒê·€ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n{analysis_summary}"}
                ]

            # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
            for message in st.session_state.messages:
                if message["role"] != "system":
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])

            # ì‚¬ìš©ì ì…ë ¥
            if prompt := st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
                # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                # AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    full_response = ""
                    
                    try:
                        stream = client.chat.completions.create(
                            model="gpt-4o-mini", # ë˜ëŠ” gpt-3.5-turbo, gpt-4 ë“±
                            messages=[
                                {"role": m["role"], "content": m["content"]}
                                for m in st.session_state.messages
                            ],
                            stream=True,
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content is not None:
                                full_response += chunk.choices[0].delta.content
                                message_placeholder.markdown(full_response + "â–Œ")
                        
                        message_placeholder.markdown(full_response)
                        
                        # ì‘ë‹µ ì €ì¥
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                        
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

except Exception as e:
    st.error(f"ë°ì´í„° ë¡œë“œ ë˜ëŠ” ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
